{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution\n",
    "- Use Kernel (small martrix of weights) to slide over the 2D mage\n",
    "- Perform element wise multiplication with the overlapping part\n",
    "- Sum up the results into a single output pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "- Spacing between position of kernel\n",
    "- See stride = 2 below in the animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/588/1*BMngs93_rm2_BpJFH2mS0Q.gif\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convolve the filter with our image by sliding the 3x3 filter image over the 6x6 image and in each position we multiply the overlapping \"pixels\", sum them up and take the sum as a value of the resulting image in that position. We then move the filter one cell to the right and repeat the process. When we're done with the first row, we move to the second row, etc... Let's calculate a couple of values together and then you'll do the rest on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Convolution.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now continue on your own until you have covered your whole original image with your filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can check the result in `images/convolved.jpeg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the size of the resulting image, if the size of the original image is `n x n` and the size of the filter is `f x f`?\n",
    "\n",
    "**A**: `n-f+1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just ran your first edge-detection algorithm! \n",
    "\n",
    "That is basically how convolutional neural networks work in very simple terms. Except, we don't specify the filters for them to use, filters are the parameters (weights) that they learn (your `w`s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "- Add extra pixels to edges to account for patterns in edge pixels\n",
    "- We've seen that the resulting image is smaller than the original image. Pixels on the edges get visited by the filter fewer times than the pixels in the middle which isn't always desireable. To get around that, we sometimes use padding, which does exactly what the name suggest, pads the original image so that the resulting image is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much padding to use?\n",
    "\n",
    "Two common options:\n",
    "\n",
    "* `Valid`: no padding\n",
    "* `Same`: preserves image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the size of the resulting image, if the size of the original image is `n x n`, the size of the filter is `f x f`, and we use `p` pixels of padding on each side?\n",
    "\n",
    "**A**: `n-f+1+2p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the of the resulting image, if the size of the original image is `n x n`, the size of the filter is `f x f`, we use padding of `p` and stride of `f`?\n",
    "\n",
    "**A**: `(n-f+2p)/s + 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to convolutional layers that we saw above, in CNN we also have _pooling layers_. Pooling accumulates/pools together the features created in convolutional layers.\n",
    "\n",
    "Max pooling, the most common pooling method, selects only the highest value in a given pooling window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![max_pooling.png](images/max_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other type of Pooling - Average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color (RGB) image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a color (RGB) image we start with three filters corresponding to each color channel, perform convolution like before, and then sum up three channels to form the resulting image that is now 4x4x1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One layer of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rgb.png](images/rgb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CNNs we generally use more than one filter in each layer, so the third dimension of the resulting image will be equal to the number of filters used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rgb_2.png](images/rgb_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diagrams of CNNS, this is usually represented in this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one_layer.png](images/one_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on what are image kernels\n",
    "\n",
    "Spend some time looking at this explanation of [Image Kernels](https://setosa.io/ev/image-kernels/) (feel free to play around with all the settings!) and discuss the following questions:\n",
    "- What does the number at each pixel location in a greyscale image mean?\n",
    "- What is a kernel?\n",
    "- How is a kernel applied to an image?\n",
    "- How does changing the numbers in the kernel affect the output image?\n",
    "- What features of an image can we highlight using a kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical architecture of CNNs has a couple of convolutional layers, followed by _fully connected_ layer, i.e. feed forward NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some of the most famous CNN architectures it is common to alternate convolutional and pooling layers, like this. Shown below is one of the classical CNN architecture, LeNet-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn_with_pooling.png](images/cnn_with_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of filters:\n",
    "* lowers the number of parameters\n",
    "* makes sense because a filter used for detecting a particular feature could be useful in multiple areas of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixels close by are related:\n",
    "* flattening the image would lose some of this information\n",
    "* pixels far away aren't: we don't need to connect each of the pixels of the input with the each of the pixels of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of CNN\n",
    "https://adamharley.com/nn_vis/cnn/2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other layers to reduce overfitting\n",
    "- Batchnormalization\n",
    "    - Batch normalization is a method used to make artificial neural networks faster and more stable \n",
    "    through normalization of the layers' inputs by re-centering and re-scaling.\n",
    "- Dropout\n",
    "    - Dropout is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. It is an efficient way of performing model averaging with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Flatten\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting mnist data present within keras dataset into train and test\n",
    "(xtrain, ytrain), (xtest, ytest) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape to miror a set of: (images, w,h,channels)\n",
    "(For using a 2D convolution we need the input shape to have 3 dimensions which are width,height,number_of_color_channels)\n",
    "Note: Color images have 3 color channels (RGB) , black and white image has 1 color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.reshape(60000,28,28,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(60000,28,28,1)\n",
    "xtest =xtest.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the mnist dataset we have 10 classes to categorize, ie numbers from 0-9\n",
    "np.unique(ytrain) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a copy of ytest, for more details (https://docs.python.org/3/library/copy.html)\n",
    "# we do this because we want to keep the original labels and another variable with the labels encoded\n",
    "ytest_true = ytest.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical(ytest) # eg. of how to one-hot-encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = to_categorical(ytrain)\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lets define the model and try to create it in tensorflow\n",
    "- Convolutional layer with 6 filters \n",
    "    - Maxpooling\n",
    "    - ReLu Activation\n",
    "- Convolutional layer with 16 filters \n",
    "    - Maxpooling\n",
    "    - ReLu Activation\n",
    "- Flatten()\n",
    "- Dense layer with 10 neurons\n",
    "- Softmax activation for the classes\n",
    "\n",
    "Add all layers within the Sequential block while constructing CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tensorflow model with the above definition\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session() # clear the cache of model parameters\n",
    "model = Sequential([\n",
    "    Conv2D(filters=6,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu',input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2,2),strides=2),\n",
    "    \n",
    "    Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2),strides=2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(units=10,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference for CNN hyperparameter layers\n",
    "- [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
    "- [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D)\n",
    "- [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)\n",
    "- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "\n",
    "The number of units in the last Dense layer will be the number of classes for classification\n",
    "For binary classification use sigmoid as the activation function in the last layer else for multiclass classification use softmax.\n",
    "\n",
    "For the activation function that can be used in the intermediate layers refer [activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    "\n",
    "The [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation function can be used in the intermediate layers with most types of neural networks. It is recommended as the default for both Multilayer Perceptron (MLP) and Convolutional Neural Networks (CNNs)\n",
    "\n",
    "[General guidelines for constructing CNN models and hyperparameter tuning](https://medium.com/data-science/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7)\n",
    "\n",
    "[Hyperparamter tuning using keras tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compile the model\n",
    "- optimizer: 'rmsprop'\n",
    "- loss\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference for Hyperparameter to be used while compiling the model\n",
    "- [Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) \n",
    "    - the [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer is a popular and effective optimization algorithm for training deep learning models\n",
    "    - Training with a low learning rate can also improve model performance eg.lr = 0.001, 0.0001 ...\n",
    "- [Losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "    - For binary classification problems use [binary_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
    "    - For multi-class classification problems use [categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)\n",
    "    - For regression problems use any of the error metrics eg [mean_squared_error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)\n",
    "- [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit the model\n",
    "- epochs = 10 \n",
    "- batch size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset.\n",
    "    - typially you stop training before overfitting your model or untill there is no improvement in the validation error loss\n",
    "- The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the modelâ€™s internal parameters are updated.\n",
    "    - batch size is typically given in powers of 2 for better computation efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accuracy_metrics = model.fit(xtrain,ytrain,batch_size=128,epochs=epochs,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(accuracy_metrics.history['accuracy'],label='train_accuracy')\n",
    "plt.plot(accuracy_metrics.history['val_accuracy'],label='val_acuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"At the end of the {epochs}th epoch the validation accuracy has reached {'{:.4f}'.format(accuracy_metrics.history['val_accuracy'][-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(accuracy_metrics.history['loss'],label='train_loss')\n",
    "plt.plot(accuracy_metrics.history['val_loss'],label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"At the end of the {epochs}th epoch the validation loss has decreased to {'{:.4f}'.format(accuracy_metrics.history['val_loss'][-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example plot of overfitting\n",
    "![](https://i.stack.imgur.com/FkPkr.png)\n",
    "\n",
    "Overfitting is best judged by looking at validation loss, rather than accuracy, for a series of reasons including the fact that accuracy is not always a good way to estimate the performance of classification models\n",
    "\n",
    "In the above plot you can clearly observe that the validation loss significantly increases as you train for more epochs when compared to training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of one image from the test dataset\n",
    "plt.imshow(xtest[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original encoded label corresponding to the test data\n",
    "ytest[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict your test data\n",
    "pred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred` gives you the probability of all the 10 classes for each data in `xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[8].argmax() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the probabilities that the test data `xtest[8]` could be classified under, the 5th index has the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_true[8] # the true label for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "plt.figure(figsize = (8,8))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true=ytest_true, y_pred=np.argmax(pred, axis=-1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=np.unique(ytest_true))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- Try out varying the different hyperparameters and run the model\n",
    "- Try changing no. of filters, kernel_size, padding, activation, pool_size, units etc\n",
    "- you can also add more Conv2D and Dense layers as well\n",
    "- Try using different activation functions in the Conv2D layers\n",
    "- While compiling the model try using different optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: take a look at the Feature Maps\n",
    "#### This is to visualize the internal representation of our image as it passes through the 6 filters in our first Conv2D layer in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from numpy import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the layers present in the created model\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine model to output right after the first hidden layer\n",
    "K.clear_session()\n",
    "model_small = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "model_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image with the required shape\n",
    "img = xtrain[2]\n",
    "plt.imshow(img.reshape(28,28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand dimensions so that it represents a single 'sample' (number of images,width,height,number of channels)\n",
    "img = expand_dims(img, axis=0)\n",
    "#img = expand_dims(img, axis=3)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature map for first hidden layer\n",
    "feature_maps = model_small.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all 6 maps in an 3*2 squares\n",
    "height = 3\n",
    "width = 2\n",
    "ix = 1\n",
    "plt.figure(figsize=(12,4))\n",
    "for _ in range(height):\n",
    "    for _ in range(width):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(height, width, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "If you observe overfitting while training your model try to use some regularization\n",
    "- [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/)\n",
    "    - Add batch normalization layer in between CNN layers\n",
    "- [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "    - Add dropout layer in between dense layers\n",
    "    \n",
    "\n",
    "- Try using [callback functions](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks), these are special utilities or functions that are executed during training at given stages of the training procedure.      \n",
    "    - Callbacks can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc.\n",
    "\n",
    "    - eg. [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) which can be used to stop training when a monitored metric has stopped improving.\n",
    "`EarlyStopping(monitor='val_loss', patience=5)`\n",
    " Remember to set the hyperparameter callbacks in the `model.fit(...,callbacks=[callback])`\n",
    "\n",
    "- [Animations reference](https://medium.com/data-science/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "- [Calculate CNN parameters](https://stackoverflow.com/questions/42786717/how-to-calculate-the-number-of-parameters-for-convolutional-neural-network)\n",
    "- [Overview of all optimizers](https://www.ruder.io/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
